#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Model Arena: Base vs SFT vs DPO
åŠŸèƒ½ï¼šå®æ—¶å¯¹æ¯”ä¸‰ä¸ªé˜¶æ®µæ¨¡å‹çš„å›ç­”æ•ˆæœã€‚
ç‰¹ç‚¹ï¼šè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æƒé‡ã€å†…å­˜ä¸­åŠ¨æ€åˆå¹¶ DPO ä¾èµ–ã€‚
"""

import torch
import sys
import os
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# === é…ç½®åŒºåŸŸ ===
# 1. åŸºåº§æ¨¡å‹è·¯å¾„ (è¯·ç¡®ä¿æ­£ç¡®)
BASE_MODEL_PATH = "/root/autodl-tmp/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28"

# 2. SFT å’Œ DPO çš„è¾“å‡ºæ ¹ç›®å½•
SFT_ROOT = "outputs/sft"
DPO_ROOT = "outputs/align"
# =================

def get_latest_adapter(base_dir: str) -> str:
    """è‡ªåŠ¨å¯»æ‰¾ base_dir/times ä¸‹æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•"""
    path = Path(base_dir)
    times_path = path / "times"
    
    if not times_path.exists():
        return str(path)
    
    subdirs = [d for d in times_path.iterdir() if d.is_dir()]
    if not subdirs:
        return str(path)
        
    latest = sorted(subdirs, key=lambda x: x.name, reverse=True)[0]
    return str(latest)

def load_models():
    print(f"\n[1/4] æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹...")
    print(f"      è·¯å¾„: {BASE_MODEL_PATH}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    except Exception as e:
        print(f"âŒ åŸºåº§æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

    # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°è·¯å¾„
    sft_path = get_latest_adapter(SFT_ROOT)
    dpo_path = get_latest_adapter(DPO_ROOT)

    print(f"[2/4] åŠ è½½ SFT Adapter (ç”¨äºå¾®è°ƒæ•ˆæœ)...")
    print(f"      è·¯å¾„: {sft_path}")
    try:
        # åŠ è½½ SFT é€‚é…å™¨ï¼Œå‘½åä¸º 'sft'
        model = PeftModel.from_pretrained(base_model, sft_path, adapter_name="sft")
    except Exception as e:
        print(f"âŒ SFT åŠ è½½å¤±è´¥: {e}")
        return None, None

    print(f"[3/4] åŠ è½½ DPO Adapter (ç”¨äºå¯¹é½æ•ˆæœ)...")
    print(f"      è·¯å¾„: {dpo_path}")
    try:
        # åŠ è½½ DPO é€‚é…å™¨ï¼Œå‘½åä¸º 'dpo'
        model.load_adapter(dpo_path, adapter_name="dpo")
    except Exception as e:
        print(f"âŒ DPO åŠ è½½å¤±è´¥: {e}")
        print("      (å¯èƒ½ä½ è¿˜æ²¡è·‘å®Œ DPOï¼Œæˆ–è€…è·¯å¾„ä¸å¯¹)")
        return None, None
        
    return model, tokenizer

def generate_response(model, tokenizer, prompt, mode):
    """
    mode: 'base', 'sft', 'dpo'
    """
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    try:
        # === æ ¸å¿ƒåˆ‡æ¢é€»è¾‘ ===
        if mode == "base":
            # 1. åŸºåº§: ç¦ç”¨æ‰€æœ‰é€‚é…å™¨
            with model.disable_adapter():
                with torch.no_grad():
                    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7, top_p=0.9, do_sample=True)

        elif mode == "sft":
            # 2. SFT: å¯ç”¨ sft é€‚é…å™¨
            model.set_adapter("sft")
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7, top_p=0.9, do_sample=True)

        elif mode == "dpo":
            # 3. DPO: éœ€è¦ SFT çš„åº•å­
            # æ­¥éª¤ A: æ¿€æ´» SFT å¹¶åˆå¹¶è¿›åŸºåº§ (Base å˜æˆ Base+SFT)
            model.set_adapter("sft")
            model.merge_adapter()
            
            # æ­¥éª¤ B: æ¿€æ´» DPO (ç°åœ¨æ˜¯ Base+SFT+DPO)
            model.set_adapter("dpo")
            
            # æ­¥éª¤ C: ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7, top_p=0.9, do_sample=True)
            
            # æ­¥éª¤ D: æ¢å¤ç°åœº (éå¸¸é‡è¦!)
            # å¿…é¡»å…ˆåˆ‡å› sft æ‰èƒ½è§£åˆå¹¶
            model.set_adapter("sft")
            model.unmerge_adapter() 
            
    except Exception as e:
        # å¼‚å¸¸æ¢å¤ï¼Œé˜²æ­¢å½±å“ä¸‹ä¸€è½®
        try:
            model.set_adapter("sft")
            model.unmerge_adapter()
        except: pass
        return f"[Error] {str(e)}"

    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs)]
    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

def main():
    print("="*60)
    print("âš”ï¸  LLM ç«æŠ€åœºï¼šBase vs SFT vs DPO  âš”ï¸")
    print("="*60)
    
    model, tokenizer = load_models()
    if not model: return

    print("\nâœ… ç¯å¢ƒå°±ç»ªï¼è¾“å…¥ 'exit' é€€å‡ºã€‚")
    
    while True:
        try:
            query = input("\nğŸ¤ è¯·è¾“å…¥é—®é¢˜: ").strip()
        except EOFError: break
        
        if not query: continue
        if query.lower() in ["exit", "quit"]: break

        print("-" * 60)
        
        # ä¾æ¬¡ç”Ÿæˆä¸‰ä¸ªæ¨¡å‹çš„å›ç­”
        for name, label in [("base", "ğŸ”µ Base (åŸºåº§)"), ("sft", "ğŸŸ¢ SFT (å¾®è°ƒ)"), ("dpo", "ğŸŸ£ DPO (å¯¹é½)")]:
            print(f"{label} æ€è€ƒä¸­...", end="", flush=True)
            res = generate_response(model, tokenizer, query, name)
            print(f"\r{label}:\n{res}\n")
            print("-" * 30)

if __name__ == "__main__":
    main()