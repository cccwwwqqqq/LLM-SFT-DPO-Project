#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Model Arena: Compare Base vs SFT vs DPO in real-time.
(Fixed: Use context manager for Base model to prevent state lock-up)
"""

import torch
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# === é…ç½®åŒºåŸŸ ===
PATHS = {
    # 1. åŸºåº§æ¨¡å‹
    "base": "/root/autodl-tmp/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28",
    
    # 2. SFT Adapter
    "sft": "outputs/sft",
    
    # 3. DPO Adapter
    "dpo": "outputs/align/times/20260206_214733"
}
# ==========================================

def load_models():
    print(f"\n[1/4] æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹: {PATHS['base']} ...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(PATHS['base'], trust_remote_code=True)
        base_model = AutoModelForCausalLM.from_pretrained(
            PATHS['base'],
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    except Exception as e:
        print(f"âŒ åŠ è½½åŸºåº§æ¨¡å‹å¤±è´¥: {e}")
        return None, None

    print(f"[2/4] æ­£åœ¨åŠ è½½ SFT Adapter: {PATHS['sft']} ...")
    try:
        # åŠ è½½ç¬¬ä¸€ä¸ª Adapterï¼Œå‘½åä¸º sft
        model = PeftModel.from_pretrained(base_model, PATHS['sft'], adapter_name="sft")
    except Exception as e:
        print(f"âŒ Error loading SFT: {e}")
        return None, None

    print(f"[3/4] æ­£åœ¨åŠ è½½ DPO Adapter: {PATHS['dpo']} ...")
    try:
        # åŠ è½½ç¬¬äºŒä¸ª Adapterï¼Œå‘½åä¸º dpo
        model.load_adapter(PATHS['dpo'], adapter_name="dpo")
    except Exception as e:
        print(f"âŒ Error loading DPO: {e}")
        return None, None
        
    return model, tokenizer

def generate_response(model, tokenizer, prompt, adapter_name):
    """
    ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥å¤„ç† Base æ¨¡å‹ï¼Œç¡®ä¿çŠ¶æ€è‡ªåŠ¨æ¢å¤
    """
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # æ ¸å¿ƒä¿®å¤é€»è¾‘
    try:
        if adapter_name == "base":
            # ã€é‡è¦ã€‘ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æš‚æ—¶ç¦ç”¨ Adapter
            # è·‘å®Œè¿™è¡Œä»£ç åï¼ŒAdapter ä¼šè‡ªåŠ¨é‡æ–°å¼€å¯ï¼Œä¸ä¼šå¯¼è‡´åç»­æŠ¥é”™
            with model.disable_adapter():
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=512,
                        temperature=0.7,
                        top_p=0.9,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id
                    )
        else:
            # åˆ‡æ¢åˆ°æŒ‡å®šçš„ Adapter
            model.set_adapter(adapter_name)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id
                )
    except Exception as e:
        return f"[ç³»ç»Ÿé”™è¯¯] ç”Ÿæˆå¤±è´¥: {str(e)}"

    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs)]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def main():
    print("="*60)
    print("âš”ï¸  LLM ç«æŠ€åœºï¼šBase vs SFT vs DPO  âš”ï¸")
    print("="*60)
    
    model, tokenizer = load_models()
    if not model:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    print("\nâœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")
    print("è¾“å…¥ 'exit' é€€å‡ºã€‚\n")

    while True:
        try:
            query = input("\nğŸ¤ è¯·è¾“å…¥æµ‹è¯•é—®é¢˜: ").strip()
        except EOFError:
            break
        
        if not query: continue
        if query.lower() in ["exit", "quit"]: break

        print("-" * 60)
        
        # 1. Base
        print("ğŸ”µ [Base åŸºåº§] æ€è€ƒä¸­...", end="", flush=True)
        res_base = generate_response(model, tokenizer, query, "base")
        print(f"\rğŸ”µ [Base åŸºåº§]:\n{res_base}\n")
        print("-" * 30)

        # 2. SFT
        print("ğŸŸ¢ [SFT å¾®è°ƒ] æ€è€ƒä¸­...", end="", flush=True)
        res_sft = generate_response(model, tokenizer, query, "sft")
        print(f"\rğŸŸ¢ [SFT å¾®è°ƒ]:\n{res_sft}\n")
        print("-" * 30)

        # 3. DPO
        print("ğŸŸ£ [DPO å¯¹é½] æ€è€ƒä¸­...", end="", flush=True)
        res_dpo = generate_response(model, tokenizer, query, "dpo")
        print(f"\rğŸŸ£ [DPO å¯¹é½]:\n{res_dpo}")
        print("-" * 60)

if __name__ == "__main__":
    main()